{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Sentence Transformer Model\n",
        "\n",
        "In this task, I build a basic sentence transformer model.  \n",
        "The model loads a pre-trained transformer (DistilBERT) from Hugging Face's library to encode input sentences into contextual embeddings.\n",
        "\n",
        "After obtaining token-level embeddings, I apply mean pooling across the sequence dimension to generate a fixed-size sentence representation for each input sentence.\n",
        "\n",
        "This forms the foundation for downstream tasks like classification, similarity, or clustering.\n",
        "\n",
        "Key components:\n",
        "- **Tokenizer**: Converts input text into model-readable format (tokens, attention masks).\n",
        "- **Encoder**: Pre-trained transformer that generates contextual embeddings.\n",
        "- **Pooling Layer**: Mean pooling over token embeddings to create a sentence-level vector.\n",
        "\n",
        "I now proceed to define the Sentence Transformer model.\n",
        "\n",
        "Note: In this assignment, I focus on verifying the basic model functionality rather than robustness to noisy inputs. Handling special tokenization edge cases (e.g., extra spaces, special characters) would be part of a production-grade system but is not necessary for the current scope.\n"
      ],
      "metadata": {
        "id": "VTRy2XD6vJSM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meH6hKM2iJMu",
        "outputId": "8bf3761b-3150-4c39-9432-50bae095e570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentence-transformers torch --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Define the basic Sentence Transformer Model\n",
        "class SentenceTransformerModel(nn.Module):\n",
        "    def __init__(self, model_name='distilbert-base-uncased'):\n",
        "        super(SentenceTransformerModel, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            model_output = self.encoder(**encoded_input)\n",
        "\n",
        "        embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "        return embeddings\n"
      ],
      "metadata": {
        "id": "vuNuCYQWvZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformerModel()\n",
        "sentences = [\"The cat sat on the mat.\", \"Artificial intelligence is fascinating.\"]\n",
        "embeddings = model(sentences)\n",
        "print(\"Embedding shape:\", embeddings.shape)\n",
        "print(\"First 5 elements of the first embedding:\", embeddings[0][:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzj61fR5xmzZ",
        "outputId": "f4f9bfb8-1c4f-440e-d815-2483b6192f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([2, 768])\n",
            "First 5 elements of the first embedding: tensor([-0.0689, -0.0890, -0.0271,  0.2593,  0.0277])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Multi-Task Learning Expansion\n",
        "\n",
        "In this task, I expand the basic sentence transformer into a multi-task learning framework.  \n",
        "I add two separate task-specific heads on top of the shared transformer encoder:\n",
        "\n",
        "- **Task A**: A linear classifier for sentence classification (e.g., Finance/Health/Entertainment).\n",
        "- **Task B**: A linear classifier for sentiment analysis (positive/negative).\n",
        "\n",
        "By using a shared encoder and separate heads, I allow the model to learn both tasks simultaneously while reusing the same underlying text representations.\n",
        "\n",
        "Key components:\n",
        "- **Shared Encoder**: DistilBERT backbone generates contextual embeddings.\n",
        "- **Pooling**: Mean pooling produces fixed-size sentence embeddings.\n",
        "- **Task-specific Heads**: Separate `nn.Linear` layers for each task.\n",
        "\n",
        "I now proceed to define the Multi-Task Sentence Transformer model.\n"
      ],
      "metadata": {
        "id": "vIzOkf7Yyebl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskSentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name='distilbert-base-uncased', num_classes_A=3, num_classes_B=2):\n",
        "        super(MultiTaskSentenceTransformer, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Task-specific heads\n",
        "        self.classifier_A = nn.Linear(self.encoder.config.hidden_size, num_classes_A)\n",
        "        self.classifier_B = nn.Linear(self.encoder.config.hidden_size, num_classes_B)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            model_output = self.encoder(**encoded_input)\n",
        "\n",
        "        pooled_output = model_output.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        out_A = self.classifier_A(pooled_output)\n",
        "        out_B = self.classifier_B(pooled_output)\n",
        "\n",
        "        return out_A, out_B, pooled_output\n"
      ],
      "metadata": {
        "id": "fAeHsYX_yKKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_task_model = MultiTaskSentenceTransformer()\n",
        "test_sentences = [\"I love machine learning.\", \"The economy is crashing.\"]\n",
        "out_A, out_B, pooled_embeddings = multi_task_model(test_sentences)\n",
        "print(\"Task A output shape:\", out_A.shape)\n",
        "print(\"Task B output shape:\", out_B.shape)\n",
        "print(\"Pooled Embedding shape:\", pooled_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT4E59rdyj4q",
        "outputId": "ad6eb32b-8236-44c2-dacb-1c3c97f78284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A output shape: torch.Size([2, 3])\n",
            "Task B output shape: torch.Size([2, 2])\n",
            "Pooled Embedding shape: torch.Size([2, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulated Dataset for Multi-Task Training\n",
        "\n",
        "For the purpose of this assessment, I simulate a small labeled dataset consisting of 20 realistic sentences.  \n",
        "Each sentence is manually assigned two labels:\n",
        "\n",
        "- **Task A**: Topic classification (Finance/Technology, Health/Lifestyle, or Entertainment/Sports).\n",
        "- **Task B**: Sentiment classification (Positive or Negative).\n",
        "\n",
        "This allows me to test the multi-task learning setup without requiring a real-world dataset.\n"
      ],
      "metadata": {
        "id": "zkxmu75yy0zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences\n",
        "sentences = [\n",
        "    \"The stock market crashed yesterday.\",\n",
        "    \"Artificial intelligence is changing industries rapidly.\",\n",
        "    \"The new football season starts next month.\",\n",
        "    \"This pizza tastes amazing!\",\n",
        "    \"Climate change is affecting global temperatures.\",\n",
        "    \"I hate waiting in long lines at the airport.\",\n",
        "    \"Blockchain technology could revolutionize banking.\",\n",
        "    \"I enjoyed the concert last night!\",\n",
        "    \"The weather forecast predicts heavy rain tomorrow.\",\n",
        "    \"Electric cars are becoming more popular.\",\n",
        "    \"I failed my math test and feel terrible.\",\n",
        "    \"Space exploration has advanced significantly in recent years.\",\n",
        "    \"The new restaurant downtown serves delicious food.\",\n",
        "    \"Unemployment rates have dropped this quarter.\",\n",
        "    \"I'm feeling very sick and tired today.\",\n",
        "    \"The new smartphone release has created a lot of buzz.\",\n",
        "    \"Meditation can greatly reduce stress.\",\n",
        "    \"The movie I watched yesterday was a huge disappointment.\",\n",
        "    \"Global pandemics can disrupt economies worldwide.\",\n",
        "    \"This book offers an inspiring perspective on life.\"\n",
        "]\n",
        "\n",
        "# Task A: Topic labels (3 classes)\n",
        "# 0: Finance/Technology\n",
        "# 1: Health/Lifestyle\n",
        "# 2: Entertainment/Sports\n",
        "\n",
        "task_A_labels = torch.tensor([\n",
        "    0, 0, 2, 2, 1,\n",
        "    1, 0, 2, 1, 0,\n",
        "    1, 0, 2, 0, 1,\n",
        "    0, 1, 2, 0, 1\n",
        "])\n",
        "\n",
        "# Task B: Sentiment labels (2 classes)\n",
        "# 0: Negative\n",
        "# 1: Positive\n",
        "\n",
        "task_B_labels = torch.tensor([\n",
        "    0, 1, 1, 1, 0,\n",
        "    0, 1, 1, 0, 1,\n",
        "    0, 1, 1, 1, 0,\n",
        "    1, 1, 0, 0, 1\n",
        "])\n"
      ],
      "metadata": {
        "id": "lSZnaUwAy0Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Training Considerations and Fine-Tuning Strategy\n",
        "\n",
        "In a real-world fine-tuning setting, I would consider the following strategies:\n",
        "\n",
        "- **Freezing the Encoder Initially**: I would initially freeze the pre-trained DistilBERT encoder layers and train only the task-specific heads. This allows the new heads to stabilize without disrupting the pre-trained language representations.\n",
        "\n",
        "- **Gradual Unfreezing**: After a few epochs (e.g., 2–3), I would gradually unfreeze lower encoder layers to allow fine-tuning. This would help the model adapt better to the specific characteristics of the new dataset without catastrophic forgetting.\n",
        "\n",
        "- **Layer-wise Learning Rate Decay**: I would apply smaller learning rates to lower layers and higher learning rates to upper layers. This ensures that lower-level linguistic features are not destroyed during fine-tuning.\n",
        "\n",
        "- **Multi-Task Loss Balancing**: If tasks have very different difficulty levels or data distributions, I would consider applying weighted loss terms to balance learning between tasks.\n",
        "\n",
        "Overall, I aim to leverage transfer learning effectively while minimizing overfitting or loss of useful pre-trained information.\n",
        "\n",
        "My key decisions for training strategy included starting with a frozen encoder to stabilize task-specific heads, followed by gradual unfreezing to fine-tune the backbone while preserving pre-trained knowledge.  \n",
        "I also planned to apply layer-wise learning rate decay to protect lower layers.  \n",
        "The insight behind this approach is to leverage transfer learning optimally without causing catastrophic forgetting or overfitting to the small dataset."
      ],
      "metadata": {
        "id": "uBEIoJOg4Ak6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop Function\n",
        "\n",
        "In this step, I define a training loop function that performs:\n",
        "\n",
        "- Forward pass through the multi-task model\n",
        "- Calculation of separate losses for Task A and Task B\n",
        "- Combination of the two losses\n",
        "- Backward pass and optimizer step\n",
        "\n",
        "Since the model is designed for simulation only, I run a single epoch without batch division or validation split.\n",
        "For training simulation, I chose to use a small manually labeled dataset of 20 sentences to keep training light and within the assignment scope.  \n",
        "I designed the model to output both task-specific predictions in a shared forward pass and computed separate losses for each task.  \n",
        "While accuracies remained low due to random initialization and limited data, the simulation successfully demonstrated multi-task learning functionality, forward pass, and metric computation as intended."
      ],
      "metadata": {
        "id": "a5vd9xfm2mha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, sentences, task_A_labels, task_B_labels, optimizer, criterion_A, criterion_B):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out_A, out_B, embeddings = model(sentences)\n",
        "\n",
        "    # Compute individual losses\n",
        "    loss_A = criterion_A(out_A, task_A_labels)\n",
        "    loss_B = criterion_B(out_B, task_B_labels)\n",
        "\n",
        "    # Total loss (simple sum of two losses)\n",
        "    total_loss = loss_A + loss_B\n",
        "\n",
        "    # Backward pass\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss details\n",
        "    print(f\"Loss A (Task A): {loss_A.item():.4f}\")\n",
        "    print(f\"Loss B (Task B): {loss_B.item():.4f}\")\n",
        "    print(f\"Total Loss: {total_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "VD-pHbNQzOgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulated Training Execution\n",
        "\n",
        "I now run a single epoch of training using the simulated dataset.  \n",
        "This demonstrates the end-to-end functioning of the multi-task model, including:\n",
        "\n",
        "- Forward pass\n",
        "- Loss computation for both tasks\n",
        "- Loss backpropagation\n",
        "- Parameter updates\n"
      ],
      "metadata": {
        "id": "8KkP3YoF20du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(multi_task_model.parameters(), lr=2e-5)\n",
        "criterion_A = nn.CrossEntropyLoss()  # Task A: Topic classification\n",
        "criterion_B = nn.CrossEntropyLoss()  # Task B: Sentiment classification\n",
        "train_one_epoch(\n",
        "    multi_task_model,\n",
        "    sentences,\n",
        "    task_A_labels,\n",
        "    task_B_labels,\n",
        "    optimizer,\n",
        "    criterion_A,\n",
        "    criterion_B\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odnNJOIrzOjn",
        "outputId": "c708bc5b-1d73-46fa-ca8e-e93cd57270aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss A (Task A): 1.1295\n",
            "Loss B (Task B): 0.7134\n",
            "Total Loss: 1.8428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate training accuracy for Task A and Task B\n",
        "\n",
        "# Get model predictions\n",
        "out_A, out_B, _ = multi_task_model(sentences)\n",
        "\n",
        "preds_A = torch.argmax(out_A, dim=1)\n",
        "preds_B = torch.argmax(out_B, dim=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_A = (preds_A == task_A_labels).float().mean().item()\n",
        "accuracy_B = (preds_B == task_B_labels).float().mean().item()\n",
        "\n",
        "print(f\"Task A Training Accuracy: {accuracy_A * 100:.2f}%\")\n",
        "print(f\"Task B Training Accuracy: {accuracy_B * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXDBtPXg4pQR",
        "outputId": "17fbe0f0-167b-4d8e-f9de-662d3409a255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A Training Accuracy: 20.00%\n",
            "Task B Training Accuracy: 40.00%\n"
          ]
        }
      ]
    }
  ]
}